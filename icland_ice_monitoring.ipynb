{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Iceland Snow and Ice Monitoring\n",
        "\n",
        "This notebook implements a workflow for monitoring snow and ice in Iceland using Sentinel-2 data via the EOPF Zarr format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import geopandas as gpd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "from matplotlib.patches import Patch\n",
        "from shapely.geometry import box\n",
        "import shapely.geometry\n",
        "import pystac_client\n",
        "from pystac import Item\n",
        "import xarray as xr\n",
        "import os\n",
        "import requests\n",
        "from datetime import datetime\n",
        "from pyproj import Transformer\n",
        "import dask\n",
        "import warnings\n",
        "\n",
        "# Suppress pystac deprecation warnings (we handle deprecated items manually)\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning, module=\"pystac\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Seeds\n",
        "Load the glacier seeds (points) and define the Area of Interest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load seeds\n",
        "seeds_gdf = gpd.read_file(\"data/Iceland_Seeds.geojson\")\n",
        "\n",
        "# Reproject to WGS84 for search\n",
        "seeds_gdf = seeds_gdf.to_crs(\"EPSG:4326\")\n",
        "\n",
        "# Get bounding box in WGS84\n",
        "total_bounds = seeds_gdf.total_bounds\n",
        "bbox_4326 = list(total_bounds) # [minx, miny, maxx, maxy]\n",
        "\n",
        "# Define AOI for UTM transformation\n",
        "spatial_extent = {\n",
        "    \"west\": bbox_4326[0],\n",
        "    \"south\": bbox_4326[1],\n",
        "    \"east\": bbox_4326[2],\n",
        "    \"north\": bbox_4326[3],\n",
        "}\n",
        "\n",
        "print(f\"Bbox (EPSG:4326): {bbox_4326}\")\n",
        "\n",
        "# Convert AOI to UTM 27N (EPSG:32627) - Common for Iceland\n",
        "# The example used EPSG:32631 for Belgium. For Iceland, we use 32627.\n",
        "target_crs = \"EPSG:32627\"\n",
        "transformer = Transformer.from_crs(\"EPSG:4326\", target_crs, always_xy=True)\n",
        "\n",
        "west_utm, south_utm = transformer.transform(\n",
        "    spatial_extent[\"west\"], spatial_extent[\"south\"]\n",
        ")\n",
        "east_utm, north_utm = transformer.transform(\n",
        "    spatial_extent[\"east\"], spatial_extent[\"north\"]\n",
        ")\n",
        "\n",
        "# Spatial slice parameters (Note: y is typically north-to-south in these grids, so slice order matters)\n",
        "# We will verify the order after inspection, but typically it is slice(max_y, min_y) or slice(min_y, max_y) depending on the index.\n",
        "# The example used slice(north_utm, south_utm) for y, implying descending coordinates.\n",
        "x_slice = slice(west_utm, east_utm)\n",
        "y_slice = slice(north_utm, south_utm)\n",
        "\n",
        "print(f\"UTM Bounds ({target_crs}): West={west_utm}, South={south_utm}, East={east_utm}, North={north_utm}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## STAC Search and Data Loading\n",
        "\n",
        "This algorithm simulates a file-based workflow by processing full Sentinel-2 scenes (Sentinel-2 L2A) retrieved from the EOPF Zarr store. \n",
        "It avoids tile-based optimization and instead loads the full scene extent to compute NDSI and classify snow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# STAC Search\n",
        "catalog = pystac_client.Client.open(\"https://stac.core.eopf.eodc.eu\")\n",
        "\n",
        "# Search for Sentinel-2 L2A items\n",
        "# Adjust date range as needed.\n",
        "time_range_str = \"2025-06-01/2025-06-30\"\n",
        "\n",
        "print(f\"Searching STAC for {time_range_str} over AOI...\")\n",
        "search = catalog.search(\n",
        "    collections=[\"sentinel-2-l2a\"],\n",
        "    bbox=bbox_4326,\n",
        "    datetime=time_range_str,\n",
        ")\n",
        "\n",
        "items = list(search.items())\n",
        "print(f\"Found {len(items)} total items.\")\n",
        "\n",
        "# Separate deprecated and valid items\n",
        "deprecated_items = []\n",
        "valid_items = []\n",
        "\n",
        "for item in items:\n",
        "    if item.properties.get(\"deprecated\", False):\n",
        "        deprecated_items.append(item)\n",
        "    else:\n",
        "        valid_items.append(item)\n",
        "\n",
        "print(f\"  - Valid items: {len(valid_items)}\")\n",
        "print(f\"  - Deprecated items: {len(deprecated_items)}\")\n",
        "\n",
        "# Try to find newer versions for deprecated items via 'superseded-by' link\n",
        "replacement_items = []\n",
        "for dep_item in deprecated_items:\n",
        "    # Look for 'superseded-by' link\n",
        "    superseded_by_link = next(\n",
        "        (link for link in dep_item.links if link.rel == \"superseded-by\"), \n",
        "        None\n",
        "    )\n",
        "    if superseded_by_link:\n",
        "        try:\n",
        "            # Fetch the replacement item from the catalog\n",
        "            replacement = catalog.get_collection(dep_item.collection_id).get_item(\n",
        "                superseded_by_link.target.split(\"/\")[-1]\n",
        "            ) if hasattr(superseded_by_link, 'target') else None\n",
        "            \n",
        "            # Alternative: directly fetch via href if available\n",
        "            if replacement is None and superseded_by_link.href:\n",
        "                resp = requests.get(superseded_by_link.href)\n",
        "                if resp.status_code == 200:\n",
        "                    replacement = Item.from_dict(resp.json())\n",
        "                    \n",
        "            if replacement and not replacement.properties.get(\"deprecated\", False):\n",
        "                replacement_items.append(replacement)\n",
        "                print(f\"  Found replacement for {dep_item.id}: {replacement.id}\")\n",
        "            else:\n",
        "                print(f\"  No valid replacement found for {dep_item.id}\")\n",
        "        except Exception as e:\n",
        "            print(f\"  Could not fetch replacement for {dep_item.id}: {e}\")\n",
        "    else:\n",
        "        print(f\"  No 'superseded-by' link for deprecated item: {dep_item.id}\")\n",
        "\n",
        "# Combine valid items with replacements\n",
        "final_items = valid_items + replacement_items\n",
        "print(f\"\\nUsing {len(final_items)} items after resolving deprecations.\")\n",
        "\n",
        "# Filter items that have the 'product' asset to avoid KeyErrors\n",
        "hrefs = [item.assets[\"product\"].href for item in final_items if \"product\" in item.assets]\n",
        "\n",
        "if len(final_items) > 0:\n",
        "    print(f\"First item: {final_items[0].id}\")\n",
        "\n",
        "# Function to extract time from filename (as per example)\n",
        "def extract_time(ds):\n",
        "    date_format = \"%Y%m%dT%H%M%S\"\n",
        "    try:\n",
        "        filename = ds.encoding[\"source\"]\n",
        "        # The example splits by \"_\" and takes index 2. \n",
        "        # Typical EOPF Zarr name: S2A_MSIL2A_20200131T105251_...\n",
        "        # We need to ensure we parse the correct part.\n",
        "        basename = os.path.basename(filename)\n",
        "        parts = basename.split(\"_\")\n",
        "        # Look for the date string part (starts with 20...)\n",
        "        date_str = next((p for p in parts if p.startswith(\"20\") and \"T\" in p), None)\n",
        "        \n",
        "        if date_str:\n",
        "             # Remove fractional seconds if present or other suffixes\n",
        "             date_str = date_str.split(\".\")[0] \n",
        "             time = datetime.strptime(date_str, date_format)\n",
        "             # Use expand_dims to ensure time is treated as a dimension, not just a scalar coordinate\n",
        "             return ds.expand_dims(time=[time])\n",
        "    except Exception as e:\n",
        "        pass\n",
        "    \n",
        "    # Fallback/Default if extraction fails\n",
        "    return ds\n",
        "\n",
        "if not hrefs:\n",
        "    print(\"No items found. Skipping data loading.\")\n",
        "else:\n",
        "    print(\"Loading data via xarray (Lazy)...\")\n",
        "    \n",
        "    # We need B03 (Green) and B11 (SWIR) for NDSI (Snow).\n",
        "    # B03 is usually in r10m, B11 in r20m.\n",
        "    \n",
        "    try:\n",
        "        # Load Green Band (B03) - 10m\n",
        "        ds_b03 = xr.open_mfdataset(\n",
        "            hrefs,\n",
        "            engine=\"zarr\",\n",
        "            chunks={},\n",
        "            group=\"/measurements/reflectance/r10m\",\n",
        "            concat_dim=\"time\",\n",
        "            combine=\"nested\",\n",
        "            preprocess=extract_time,\n",
        "            mask_and_scale=True,\n",
        "        )[[\"b03\"]].sel(x=x_slice, y=y_slice)\n",
        "\n",
        "        # Load SWIR Band (B11) - 20m\n",
        "        ds_b11 = xr.open_mfdataset(\n",
        "            hrefs,\n",
        "            engine=\"zarr\",\n",
        "            chunks={},\n",
        "            group=\"/measurements/reflectance/r20m\",\n",
        "            concat_dim=\"time\",\n",
        "            combine=\"nested\",\n",
        "            preprocess=extract_time,\n",
        "            mask_and_scale=True,\n",
        "        )[[\"b11\"]].sel(x=x_slice, y=y_slice)\n",
        "\n",
        "        # Load SCL - 20m\n",
        "        ds_scl = xr.open_mfdataset(\n",
        "            hrefs,\n",
        "            engine=\"zarr\",\n",
        "            chunks={},\n",
        "            group=\"/conditions/mask/l2a_classification/r20m\",\n",
        "            concat_dim=\"time\",\n",
        "            combine=\"nested\",\n",
        "            preprocess=extract_time,\n",
        "            mask_and_scale=True,\n",
        "        )[[\"scl\"]].sel(x=x_slice, y=y_slice)\n",
        "\n",
        "        # Align grids: Resample 20m bands to 10m (B03 grid)\n",
        "        print(\"Resampling to common 10m grid...\")\n",
        "        ds_b11_interp = ds_b11.interp_like(ds_b03, method=\"nearest\")\n",
        "        ds_scl_interp = ds_scl.interp_like(ds_b03, method=\"nearest\")\n",
        "\n",
        "        # Merge into single DataCube\n",
        "        datacube = xr.merge([ds_b03, ds_b11_interp, ds_scl_interp])\n",
        "        \n",
        "        # Sort by time\n",
        "        datacube = datacube.sortby(\"time\")\n",
        "        \n",
        "        print(\"DataCube created:\")\n",
        "        print(datacube)\n",
        "        \n",
        "        # Compute NDSI and statistics\n",
        "        # We process it in memory if small enough, or keep lazy.\n",
        "        # Given the \"Simulate Native\" comment in previous code, let's process one time step as demo or all.\n",
        "        \n",
        "        # NDSI Calculation\n",
        "        green = datacube[\"b03\"]\n",
        "        swir = datacube[\"b11\"]\n",
        "        scl = datacube[\"scl\"]\n",
        "        \n",
        "        # NDSI\n",
        "        denom = (green + swir)\n",
        "        ndsi = (green - swir) / denom.where(denom != 0)\n",
        "        \n",
        "        # SCL Mask (Keep Snow(11) or clear pixels if needed, or just exclude clouds/shadows)\n",
        "        # 3: Cloud Shadows, 8: Cloud Medium, 9: Cloud High\n",
        "        valid_mask = ~scl.isin([3, 8, 9])\n",
        "        \n",
        "        # Simple Snow Map\n",
        "        snow_map = (ndsi > 0.42) & valid_mask\n",
        "        \n",
        "        # Trigger computation for the first time step to verify\n",
        "        if datacube.time.size > 0:\n",
        "            print(\"Computing first time step...\")\n",
        "            snow_map_first = snow_map.isel(time=0).compute()\n",
        "            print(f\"Snow pixels in first scene: {snow_map_first.sum().item()}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating datacube: {e}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
