{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Iceland Snow and Ice Monitoring\n",
        "\n",
        "This notebook implements a workflow for monitoring snow and ice in Iceland using Sentinel-2 data via the EOPF Zarr format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import geopandas as gpd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "from matplotlib.patches import Patch\n",
        "from shapely.geometry import box\n",
        "import shapely.geometry\n",
        "import pystac_client\n",
        "from pystac import Item\n",
        "import xarray as xr\n",
        "import os\n",
        "import requests\n",
        "from datetime import datetime\n",
        "from pyproj import Transformer\n",
        "import dask\n",
        "import warnings\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Seeds\n",
        "Load the glacier seeds (points) and define the Area of Interest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load seeds\n",
        "seeds_gdf = gpd.read_file(\"data/Iceland_Seeds.geojson\")\n",
        "\n",
        "# Reproject to WGS84 for search\n",
        "seeds_gdf = seeds_gdf.to_crs(\"EPSG:4326\")\n",
        "\n",
        "# Get bounding box in WGS84\n",
        "total_bounds = seeds_gdf.total_bounds\n",
        "bbox_4326 = list(total_bounds) # [minx, miny, maxx, maxy]\n",
        "\n",
        "# Define AOI for UTM transformation\n",
        "spatial_extent = {\n",
        "    \"west\": bbox_4326[0],\n",
        "    \"south\": bbox_4326[1],\n",
        "    \"east\": bbox_4326[2],\n",
        "    \"north\": bbox_4326[3],\n",
        "}\n",
        "\n",
        "print(f\"Bbox (EPSG:4326): {bbox_4326}\")\n",
        "\n",
        "# Convert AOI to UTM 27N (EPSG:32627) - Common for Iceland\n",
        "# The example used EPSG:32631 for Belgium. For Iceland, we use 32627.\n",
        "target_crs = \"EPSG:32627\"\n",
        "transformer = Transformer.from_crs(\"EPSG:4326\", target_crs, always_xy=True)\n",
        "\n",
        "west_utm, south_utm = transformer.transform(\n",
        "    spatial_extent[\"west\"], spatial_extent[\"south\"]\n",
        ")\n",
        "east_utm, north_utm = transformer.transform(\n",
        "    spatial_extent[\"east\"], spatial_extent[\"north\"]\n",
        ")\n",
        "\n",
        "# Spatial slice parameters (Note: y is typically north-to-south in these grids, so slice order matters)\n",
        "# We will verify the order after inspection, but typically it is slice(max_y, min_y) or slice(min_y, max_y) depending on the index.\n",
        "# The example used slice(north_utm, south_utm) for y, implying descending coordinates.\n",
        "x_slice = slice(west_utm, east_utm)\n",
        "y_slice = slice(north_utm, south_utm)\n",
        "\n",
        "print(f\"UTM Bounds ({target_crs}): West={west_utm}, South={south_utm}, East={east_utm}, North={north_utm}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## STAC Search and Data Loading\n",
        "\n",
        "This algorithm simulates a file-based workflow by processing full Sentinel-2 scenes (Sentinel-2 L2A) retrieved from the EOPF Zarr store. \n",
        "It avoids tile-based optimization and instead loads the full scene extent to compute NDSI and classify snow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Configuration ===\n",
        "SINGLE_SCENE_MODE = True  # Toggle: True = only first scene, False = all scenes\n",
        "\n",
        "# STAC Search\n",
        "catalog = pystac_client.Client.open(\"https://stac.core.eopf.eodc.eu\")\n",
        "\n",
        "# Search for Sentinel-2 L2A items\n",
        "time_range_str = \"2025-09-01/2025-09-03\"\n",
        "\n",
        "print(f\"Searching STAC for {time_range_str} over AOI...\")\n",
        "search = catalog.search(\n",
        "    collections=[\"sentinel-2-l2a\"],\n",
        "    bbox=bbox_4326,\n",
        "    datetime=time_range_str,\n",
        ")\n",
        "\n",
        "items = list(search.items())\n",
        "print(f\"Found {len(items)} items.\")\n",
        "\n",
        "# Filter: only non-deprecated items with 'product' asset\n",
        "valid_items = [\n",
        "    item for item in items \n",
        "    if not item.properties.get(\"deprecated\", False) and \"product\" in item.assets\n",
        "]\n",
        "print(f\"Valid items with product asset: {len(valid_items)}\")\n",
        "\n",
        "# Get hrefs for file-based processing\n",
        "hrefs = [item.assets[\"product\"].href for item in valid_items]\n",
        "\n",
        "# Apply single scene toggle\n",
        "if SINGLE_SCENE_MODE and hrefs:\n",
        "    hrefs = [hrefs[0]]\n",
        "    print(f\"\\n[SINGLE_SCENE_MODE] Using only first scene: {os.path.basename(hrefs[0])}\")\n",
        "elif hrefs:\n",
        "    print(f\"\\nTotal scenes to process: {len(hrefs)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## File-Based Scene Processing\n",
        "\n",
        "Load and process each Sentinel-2 scene individually. This simulates a file-based workflow where each scene is processed one at a time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_scene(href, x_slice, y_slice):\n",
        "    \"\"\"\n",
        "    Load a single Sentinel-2 scene from EOPF Zarr store.\n",
        "    Returns bands B03 (Green), B11 (SWIR), and SCL mask aligned to 10m grid.\n",
        "    \"\"\"\n",
        "    scene_name = os.path.basename(href.rstrip(\"/\"))\n",
        "    print(f\"Loading scene: {scene_name}\")\n",
        "    \n",
        "    # Load Green Band (B03) - 10m resolution\n",
        "    ds_b03 = xr.open_zarr(href, group=\"/measurements/reflectance/r10m\")[[\"b03\"]]\n",
        "    ds_b03 = ds_b03.sel(x=x_slice, y=y_slice)\n",
        "    \n",
        "    # Load SWIR Band (B11) - 20m resolution\n",
        "    ds_b11 = xr.open_zarr(href, group=\"/measurements/reflectance/r20m\")[[\"b11\"]]\n",
        "    ds_b11 = ds_b11.sel(x=x_slice, y=y_slice)\n",
        "    \n",
        "    # Load SCL (Scene Classification) - 20m resolution\n",
        "    ds_scl = xr.open_zarr(href, group=\"/conditions/mask/l2a_classification/r20m\")[[\"scl\"]]\n",
        "    ds_scl = ds_scl.sel(x=x_slice, y=y_slice)\n",
        "    \n",
        "    # Resample 20m bands to 10m grid (align to B03)\n",
        "    ds_b11_interp = ds_b11.interp_like(ds_b03, method=\"nearest\")\n",
        "    ds_scl_interp = ds_scl.interp_like(ds_b03, method=\"nearest\")\n",
        "    \n",
        "    # Merge into single dataset\n",
        "    scene_data = xr.merge([ds_b03, ds_b11_interp, ds_scl_interp])\n",
        "    \n",
        "    # Extract datetime from filename\n",
        "    parts = scene_name.split(\"_\")\n",
        "    date_str = next((p for p in parts if p.startswith(\"20\") and \"T\" in p), None)\n",
        "    if date_str:\n",
        "        scene_data.attrs[\"datetime\"] = datetime.strptime(date_str.split(\".\")[0], \"%Y%m%dT%H%M%S\")\n",
        "    \n",
        "    scene_data.attrs[\"scene_name\"] = scene_name\n",
        "    return scene_data\n",
        "\n",
        "\n",
        "def compute_ndsi(scene_data):\n",
        "    \"\"\"\n",
        "    Compute NDSI (Normalized Difference Snow Index) and snow mask.\n",
        "    NDSI = (Green - SWIR) / (Green + SWIR)\n",
        "    Snow threshold: NDSI > 0.42\n",
        "    \"\"\"\n",
        "    green = scene_data[\"b03\"]\n",
        "    swir = scene_data[\"b11\"]\n",
        "    scl = scene_data[\"scl\"]\n",
        "    \n",
        "    # Calculate NDSI\n",
        "    denom = green + swir\n",
        "    ndsi = (green - swir) / denom.where(denom != 0)\n",
        "    \n",
        "    # Cloud mask: exclude Cloud Shadows (3), Cloud Medium (8), Cloud High (9)\n",
        "    valid_mask = ~scl.isin([3, 8, 9])\n",
        "    \n",
        "    # Snow classification (NDSI > 0.42 and valid pixel)\n",
        "    snow_mask = (ndsi > 0.42) & valid_mask\n",
        "    \n",
        "    return xr.Dataset({\n",
        "        \"ndsi\": ndsi,\n",
        "        \"snow_mask\": snow_mask,\n",
        "        \"valid_mask\": valid_mask\n",
        "    }, attrs=scene_data.attrs)\n",
        "\n",
        "\n",
        "def process_scenes(hrefs, x_slice, y_slice):\n",
        "    \"\"\"\n",
        "    Process all scenes in file-based workflow.\n",
        "    Returns list of results (one per scene).\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    \n",
        "    for i, href in enumerate(hrefs):\n",
        "        print(f\"\\n[{i+1}/{len(hrefs)}] Processing...\")\n",
        "        try:\n",
        "            # Load scene\n",
        "            scene_data = load_scene(href, x_slice, y_slice)\n",
        "            \n",
        "            # Compute NDSI and snow mask\n",
        "            result = compute_ndsi(scene_data)\n",
        "            \n",
        "            # Trigger computation\n",
        "            result = result.compute()\n",
        "            \n",
        "            # Statistics\n",
        "            snow_pixels = result[\"snow_mask\"].sum().item()\n",
        "            valid_pixels = result[\"valid_mask\"].sum().item()\n",
        "            snow_percent = (snow_pixels / valid_pixels * 100) if valid_pixels > 0 else 0\n",
        "            \n",
        "            print(f\"  → Snow pixels: {snow_pixels:,} ({snow_percent:.1f}% of valid area)\")\n",
        "            \n",
        "            results.append(result)\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"  → Error: {e}\")\n",
        "            continue\n",
        "    \n",
        "    return results\n",
        "\n",
        "\n",
        "# Process scenes\n",
        "if hrefs:\n",
        "    print(f\"Starting file-based processing of {len(hrefs)} scene(s)...\\n\")\n",
        "    results = process_scenes(hrefs, x_slice, y_slice)\n",
        "    print(f\"\\n✓ Successfully processed {len(results)} scene(s)\")\n",
        "else:\n",
        "    print(\"No scenes to process.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualization: Stacked Scene Composite\n",
        "\n",
        "Stack all processed scenes and compute median NDSI and aggregated snow mask for robust visualization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ======================================================================\n",
        "# Stack Results and Compute Composite\n",
        "# ======================================================================\n",
        "# Combines all processed scenes into a single composite using median/mean\n",
        "\n",
        "NDSI_THRESHOLD = 0.42  # Standard snow threshold\n",
        "\n",
        "def create_stacked_composite(results):\n",
        "    \"\"\"\n",
        "    Stack all scene results and compute composite metrics.\n",
        "    - Median NDSI (robust against outliers/clouds)\n",
        "    - Aggregated snow mask (snow if detected in majority of valid scenes)\n",
        "    \"\"\"\n",
        "    if not results:\n",
        "        return None\n",
        "    \n",
        "    scene_count = len(results)\n",
        "    print(f\"Stacking {scene_count} scene(s)...\")\n",
        "    \n",
        "    if scene_count == 1:\n",
        "        # Single scene: use directly\n",
        "        ndsi_median = results[0][\"ndsi\"]\n",
        "        snow_mask = results[0][\"snow_mask\"]\n",
        "        valid_mask = results[0][\"valid_mask\"]\n",
        "    else:\n",
        "        # Multiple scenes: stack along new dimension and compute median\n",
        "        ndsi_stack = xr.concat([r[\"ndsi\"] for r in results], dim=\"scene\")\n",
        "        valid_stack = xr.concat([r[\"valid_mask\"] for r in results], dim=\"scene\")\n",
        "        snow_stack = xr.concat([r[\"snow_mask\"] for r in results], dim=\"scene\")\n",
        "        \n",
        "        # Median NDSI (ignoring NaN)\n",
        "        ndsi_median = ndsi_stack.median(dim=\"scene\", skipna=True)\n",
        "        \n",
        "        # Valid mask: pixel valid in at least one scene\n",
        "        valid_mask = valid_stack.any(dim=\"scene\")\n",
        "        \n",
        "        # Snow mask from median NDSI\n",
        "        snow_mask = (ndsi_median > NDSI_THRESHOLD) & valid_mask\n",
        "    \n",
        "    # Calculate statistics\n",
        "    total_pixels = valid_mask.size\n",
        "    valid_pixels = valid_mask.sum().item()\n",
        "    snow_pixels = snow_mask.sum().item()\n",
        "    snow_percentage = (snow_pixels / valid_pixels * 100) if valid_pixels > 0 else 0\n",
        "    \n",
        "    print(f\"  → Total pixels: {total_pixels:,}\")\n",
        "    print(f\"  → Valid pixels: {valid_pixels:,}\")\n",
        "    print(f\"  → Snow pixels: {snow_pixels:,} ({snow_percentage:.1f}%)\")\n",
        "    \n",
        "    return {\n",
        "        \"ndsi_median\": ndsi_median,\n",
        "        \"snow_mask\": snow_mask,\n",
        "        \"valid_mask\": valid_mask,\n",
        "        \"scene_count\": scene_count,\n",
        "        \"snow_percentage\": snow_percentage\n",
        "    }\n",
        "\n",
        "\n",
        "# Create composite from results\n",
        "if 'results' in dir() and results:\n",
        "    composite = create_stacked_composite(results)\n",
        "else:\n",
        "    composite = None\n",
        "    print(\"No results available. Run processing cell first.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ======================================================================\n",
        "# Visualization: NDSI and Snow/Ice Classification Results\n",
        "# ======================================================================\n",
        "# Creates a 2-panel diagnostic plot:\n",
        "# 1. Median NDSI map (spectral snow index)\n",
        "# 2. Binary snow/ice mask (pixels where NDSI >= threshold)\n",
        "\n",
        "if composite:\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "    \n",
        "    # ----- Plot 1: Median NDSI -----\n",
        "    ndsi_data = composite['ndsi_median'].values\n",
        "    im1 = axes[0].imshow(ndsi_data, cmap='RdYlGn', vmin=-0.5, vmax=1.0)\n",
        "    axes[0].set_title(f'Median NDSI\\n(from {composite[\"scene_count\"]} scene(s))', \n",
        "                      fontsize=12, fontweight='bold')\n",
        "    axes[0].set_xlabel('Pixel X')\n",
        "    axes[0].set_ylabel('Pixel Y')\n",
        "    plt.colorbar(im1, ax=axes[0], label='NDSI', shrink=0.8)\n",
        "    \n",
        "    # ----- Plot 2: Snow/Ice Mask -----\n",
        "    snow_data = composite['snow_mask'].values.astype(float)\n",
        "    im2 = axes[1].imshow(snow_data, cmap='Blues', vmin=0, vmax=1)\n",
        "    axes[1].set_title(f'Snow/Ice Mask\\n(NDSI >= {NDSI_THRESHOLD})', \n",
        "                      fontsize=12, fontweight='bold')\n",
        "    axes[1].set_xlabel('Pixel X')\n",
        "    axes[1].set_ylabel('Pixel Y')\n",
        "    \n",
        "    # Add percentage annotation\n",
        "    axes[1].text(0.5, 0.95, f'{composite[\"snow_percentage\"]:.1f}% Snow Coverage', \n",
        "                 transform=axes[1].transAxes, ha='center', va='top', fontsize=11, \n",
        "                 bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(f\"\\n✓ Visualization complete\")\n",
        "    print(f\"  Scenes processed: {composite['scene_count']}\")\n",
        "    print(f\"  Snow coverage: {composite['snow_percentage']:.1f}%\")\n",
        "else:\n",
        "    print(\"No composite available. Run stacking cell first.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
