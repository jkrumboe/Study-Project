{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iceland Snow and Ice Monitoring\n",
    "\n",
    "This notebook implements a workflow for monitoring snow and ice in Iceland using Sentinel-2 data via the EOPF Zarr format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'geopandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgeopandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgpd\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'geopandas'"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib.patches import Patch\n",
    "from xcube.core.store import new_data_store\n",
    "from xcube_eopf.utils import reproject_bbox\n",
    "from shapely.geometry import box\n",
    "import shapely.geometry\n",
    "\n",
    "# Initialize Data Store\n",
    "store = new_data_store(\"eopf-zarr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seeds\n",
    "Load the glacier seeds (points) and define the Area of Interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load seeds\n",
    "seeds_gdf = gpd.read_file(\"data/Iceland_Seeds.geojson\")\n",
    "\n",
    "# Reproject to WGS84 for search\n",
    "seeds_gdf = seeds_gdf.to_crs(\"EPSG:4326\")\n",
    "\n",
    "# Create a buffer around seeds (approx 5km for 10x10km tile) or just use bounds\n",
    "# For the native algorithm, we want scenes covering these seeds.\n",
    "total_bounds = seeds_gdf.total_bounds # [minx, miny, maxx, maxy]\n",
    "print(f\"Total Bounds (EPSG:4326): {total_bounds}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentinel Native Algorithm\n",
    "\n",
    "This algorithm simulates a file-based workflow by processing full Sentinel-2 scenes (Sentinel-2 L2A) retrieved from the EOPF Zarr store. \n",
    "It avoids tile-based optimization and instead loads the full scene extent to compute NDSI and classify snow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_scene_native(scene_id, store, threshold=0.42):\n",
    "    \"\"\"\n",
    "    Process a single Sentinel-2 scene using the Native approach.\n",
    "    Loads the full scene, computes NDSI, and classifies snow.\n",
    "    \"\"\"\n",
    "    print(f\"Processing scene: {scene_id}\")\n",
    "    \n",
    "    # Open the dataset using the scene ID directly\n",
    "    # We do not provide a bbox to open_data to ensure we get the full scene extent available in the store\n",
    "    # Note: If the store requires a bbox for 'sentinel-2-l2a', we might need to fetch the scene metadata first.\n",
    "    # Assuming 'scene_id' can be passed as data_id or we filter by it.\n",
    "    \n",
    "    # Try opening as a specific product if supported, or filter collection\n",
    "    try:\n",
    "        # Attempt to open the specific product. \n",
    "        # In some EOPF stores, the data_id is the product ID.\n",
    "        ds = store.open_data(data_id=scene_id)\n",
    "    except Exception as e:\n",
    "        print(f\"Could not open {scene_id} directly: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Mask valid data using SCL (Scene Classification Layer)\n",
    "    # SCL: 8=Dark, 9=Cloud Shadows, 3=Cloud Shadows (usually 3 is cloud shadow, 8,9 are clouds/cirrus)\n",
    "    # Sentinel-2 SCL: 3=Cloud Shadows, 8=Cloud Medium Probability, 9=Cloud High Probability\n",
    "    if \"scl\" in ds:\n",
    "        valid_mask = ~ds[\"scl\"].isin([3, 8, 9])\n",
    "        ds_valid = ds.where(valid_mask)\n",
    "    else:\n",
    "        print(\"SCL layer not found, skipping cloud masking.\")\n",
    "        ds_valid = ds\n",
    "\n",
    "    # NDSI Computation\n",
    "    # NDSI = (Green - SWIR) / (Green + SWIR)\n",
    "    # Bands: Green=B03, SWIR=B11\n",
    "    if \"b03\" in ds_valid and \"b11\" in ds_valid:\n",
    "        green = ds_valid[\"b03\"]\n",
    "        swir = ds_valid[\"b11\"]\n",
    "        \n",
    "        # Load data into memory to simulate \"downloading the whole file\"\n",
    "        # This forces retrieval of all chunks for these bands\n",
    "        green.load()\n",
    "        swir.load()\n",
    "        \n",
    "        ndsi = (green - swir) / (green + swir)\n",
    "        snow_map = ndsi > threshold\n",
    "        \n",
    "        # Calculate Snow Cover Percentage\n",
    "        # Count valid pixels\n",
    "        valid_count = valid_mask.sum().compute()\n",
    "        snow_count = snow_map.where(valid_mask).sum().compute()\n",
    "        \n",
    "        if valid_count > 0:\n",
    "            snow_pct = (snow_count / valid_count) * 100\n",
    "        else:\n",
    "            snow_pct = 0\n",
    "            \n",
    "        print(f\"  Snow Cover: {snow_pct:.2f}%\")\n",
    "        return {\n",
    "            \"scene_id\": scene_id,\n",
    "            \"snow_pct\": snow_pct,\n",
    "            \"snow_area_px\": snow_count.item(),\n",
    "            \"snow_map\": snow_map # Keep in memory or save if needed\n",
    "        }\n",
    "    else:\n",
    "        print(\"  Required bands (B03, B11) not found.\")\n",
    "        return None\n",
    "\n",
    "def sentinel_native_algorithm(seeds_gdf, time_range, store):\n",
    "    \"\"\"\n",
    "    Main function for the Native Algorithm.\n",
    "    1. Search for scenes covering the seeds.\n",
    "    2. Process each scene (load full, compute NDSI).\n",
    "    3. If snow > 30%, (conceptually) expand search.\n",
    "    \"\"\"\n",
    "    bbox = list(seeds_gdf.total_bounds)\n",
    "    \n",
    "    # Search for data\n",
    "    # Placeholder: Query for IDs (Using search_data if it supports returning items, or assume a list for demo)\n",
    "    # Ideally: search_results = store.search_data(bbox=bbox, time_range=time_range, data_id=\"sentinel-2-l2a\")\n",
    "    \n",
    "    # Hint: If store.search_data doesn't support item listing, use pystac_client:\n",
    "    # from pystac_client import Client\n",
    "    # client = Client.open(\"https://earth-search.aws.element84.com/v1\") # Example URL\n",
    "    # search = client.search(collections=[\"sentinel-2-l2a\"], bbox=bbox, datetime=f\"{time_range[0]}/{time_range[1]}\")\n",
    "    # scene_ids = [item.id for item in search.items()]\n",
    "\n",
    "    # For the purpose of this algorithm implementation, we will try to find scenes.\n",
    "    # Since we don't have the exact API for searching items via 'store' in this text,\n",
    "    # we will assume we can get a list of available product IDs for the region.\n",
    "    \n",
    "    print(f\"Searching for scenes in {time_range} over {bbox}...\")\n",
    "    try:\n",
    "        # This is a hypothetical call. In real xcube-eopf usage, we might use the Opensearch or STAC client directly.\n",
    "        # Here we try to open the collection and see if we can iterate indices or use a known list.\n",
    "        # Let's assume we find a way to list IDs.\n",
    "        \n",
    "        # DEMO: Let's try to just open the collection with the bbox and time, \n",
    "        # but that gives us a mosaic/cube. \n",
    "        # To respect the \"Native\" requirement, we need individual scenes.\n",
    "        pass \n",
    "    except Exception:\n",
    "        pass\n",
    "        \n",
    "    # MOCK: List of scene IDs to demonstrate the loop\n",
    "    # In a real run, replace this with actual search results from the EOPF STAC API.\n",
    "    scene_ids = [\n",
    "        # Example IDs - these would come from a search\n",
    "        # \"S2B_MSIL2A_20250619T101559_N0500_R065_T32TPS_20250619T131415\" \n",
    "    ]\n",
    "    \n",
    "    # If list is empty, we can't proceed. \n",
    "    # However, to make this runnable if the user fills in IDs:\n",
    "    results = []\n",
    "    for scene_id in scene_ids:\n",
    "        res = process_scene_native(scene_id, store)\n",
    "        if res:\n",
    "            results.append(res)\n",
    "            \n",
    "            # Check 30% condition\n",
    "            if res['snow_pct'] > 30:\n",
    "                print(f\"  Snow cover > 30%. Marking for expanded search (logic not implemented).\")\n",
    "                \n",
    "    return results\n",
    "\n",
    "# Example Usage\n",
    "# time_range = [\"2025-06-01\", \"2025-06-30\"]\n",
    "# results = sentinel_native_algorithm(seeds_gdf, time_range, store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a333be74",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison\n",
    "Compare the native results with the tile-aware Zarr approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
