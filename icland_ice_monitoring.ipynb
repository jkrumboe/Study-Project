{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Iceland Snow and Ice Monitoring\n",
        "\n",
        "This notebook implements a workflow for monitoring snow and ice in Iceland using Sentinel-2 data via the EOPF Zarr format.\n",
        "\n",
        "---\n",
        "\n",
        "## DISCLAIMER: Known Issues\n",
        "\n",
        "**The native-like Alogrithm currently does not execute successfully due to memory constraints.**\n",
        "\n",
        "- The Sentinel-2 scenes from the EOPF Zarr store are extremely large (full scene extents)\n",
        "- Even loading a single scene causes JupyterHub to crash due to insufficient memory\n",
        "- The native-like workflow approach (loading full scenes instead of tiles) is not feasible in resource-constrained environments\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import geopandas as gpd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "from matplotlib.patches import Patch\n",
        "from shapely.geometry import box\n",
        "import shapely.geometry\n",
        "import pystac_client\n",
        "from pystac import Item\n",
        "import xarray as xr\n",
        "import os\n",
        "import requests\n",
        "from datetime import datetime\n",
        "from pyproj import Transformer\n",
        "import dask\n",
        "import warnings\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "# Install maplibre for interactive maps (if not already installed)\n",
        "import subprocess\n",
        "subprocess.run([\"pip\", \"install\", \"-q\", \"maplibre\"], check=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Seeds\n",
        "Load the glacier seeds (points) and define the Area of Interest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load seeds\n",
        "seeds_gdf = gpd.read_file(\"data/Iceland_Seeds.geojson\")\n",
        "\n",
        "# Reproject to WGS84 for search\n",
        "seeds_gdf = seeds_gdf.to_crs(\"EPSG:4326\")\n",
        "\n",
        "# Get bounding box in WGS84\n",
        "total_bounds = seeds_gdf.total_bounds\n",
        "bbox_4326 = list(total_bounds) # [minx, miny, maxx, maxy]\n",
        "\n",
        "# Define AOI for UTM transformation\n",
        "spatial_extent = {\n",
        "    \"west\": bbox_4326[0],\n",
        "    \"south\": bbox_4326[1],\n",
        "    \"east\": bbox_4326[2],\n",
        "    \"north\": bbox_4326[3],\n",
        "}\n",
        "\n",
        "print(f\"Bbox (EPSG:4326): {bbox_4326}\")\n",
        "\n",
        "# Convert AOI to UTM 27N (EPSG:32627) - Common for Iceland\n",
        "# The example used EPSG:32631 for Belgium. For Iceland, we use 32627.\n",
        "target_crs = \"EPSG:32627\"\n",
        "transformer = Transformer.from_crs(\"EPSG:4326\", target_crs, always_xy=True)\n",
        "\n",
        "west_utm, south_utm = transformer.transform(\n",
        "    spatial_extent[\"west\"], spatial_extent[\"south\"]\n",
        ")\n",
        "east_utm, north_utm = transformer.transform(\n",
        "    spatial_extent[\"east\"], spatial_extent[\"north\"]\n",
        ")\n",
        "\n",
        "# Spatial slice parameters (Note: y is typically north-to-south in these grids, so slice order matters)\n",
        "# We will verify the order after inspection, but typically it is slice(max_y, min_y) or slice(min_y, max_y) depending on the index.\n",
        "# The example used slice(north_utm, south_utm) for y, implying descending coordinates.\n",
        "x_slice = slice(west_utm, east_utm)\n",
        "y_slice = slice(north_utm, south_utm)\n",
        "\n",
        "print(f\"UTM Bounds ({target_crs}): West={west_utm}, South={south_utm}, East={east_utm}, North={north_utm}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## STAC Search and Data Loading\n",
        "\n",
        "This section connects to the EOPF STAC catalog and searches for Sentinel-2 L2A scenes within a specified time range and bounding box.\n",
        "\n",
        "**What this code does:**\n",
        "- Opens a connection to the EOPF STAC catalog (`stac.core.eopf.eodc.eu`)\n",
        "- Searches for Sentinel-2 Level-2A products within the defined time range and AOI\n",
        "- Filters out deprecated items and extracts product URLs (hrefs)\n",
        "- In `SINGLE_SCENE_MODE`, selects a specific scene for testing\n",
        "\n",
        "**Algorithm approach:**\n",
        "This algorithm simulates a **file-based workflow** by processing full Sentinel-2 scenes retrieved from the EOPF Zarr store. It avoids tile-based optimization and instead loads the full scene extent to compute NDSI and classify snow.\n",
        "\n",
        "> **Warning:** This approach is memory-intensive. Loading full scenes may exceed available RAM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Configuration ===\n",
        "SINGLE_SCENE_MODE = True  # Toggle: True = specific scene, False = all scenes\n",
        "SINGLE_SCENE_NAME = \"S2B_MSIL2A_20250730T131259_N0511_R081_T26VPR_20250730T150901.zarr\"\n",
        "    \n",
        "# STAC Search\n",
        "catalog = pystac_client.Client.open(\"https://stac.core.eopf.eodc.eu\")\n",
        "\n",
        "# Search for Sentinel-2 L2A items\n",
        "time_range_str = \"2025-07-01/2025-07-31\"\n",
        "\n",
        "print(f\"Searching STAC for {time_range_str} over AOI...\")\n",
        "search = catalog.search(\n",
        "    collections=[\"sentinel-2-l2a\"],\n",
        "    bbox=bbox_4326,\n",
        "    datetime=time_range_str,\n",
        ")\n",
        "\n",
        "items = list(search.items())\n",
        "print(f\"Found {len(items)} items.\")\n",
        "\n",
        "# Filter: only non-deprecated items with 'product' asset\n",
        "valid_items = [\n",
        "    item for item in items \n",
        "    if not item.properties.get(\"deprecated\", False) and \"product\" in item.assets\n",
        "]\n",
        "print(f\"Valid items with product asset: {len(valid_items)}\")\n",
        "\n",
        "# Get hrefs for file-based processing\n",
        "hrefs = [item.assets[\"product\"].href for item in valid_items]\n",
        "\n",
        "# Apply single scene toggle\n",
        "if SINGLE_SCENE_MODE:\n",
        "    # Find the specific scene\n",
        "    matching = [h for h in hrefs if SINGLE_SCENE_NAME in h]\n",
        "    if matching:\n",
        "        hrefs = [matching[0]]\n",
        "        print(f\"\\n[SINGLE_SCENE_MODE] Using specific scene: {SINGLE_SCENE_NAME}\")\n",
        "    elif hrefs:\n",
        "        hrefs = [hrefs[0]]\n",
        "        print(f\"\\n[SINGLE_SCENE_MODE] Specific scene not found, using first: {os.path.basename(hrefs[0])}\")\n",
        "elif hrefs:\n",
        "    print(f\"\\nTotal scenes to process: {len(hrefs)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Native-Like Scene Processing\n",
        "\n",
        "### What is a \"Native-Like\" Workflow?\n",
        "\n",
        "In traditional satellite image processing, scientists download entire image files to their computer and process them locally. This is how algorithms were originally designed before cloud computing existed.\n",
        "\n",
        "**This notebook simulates that traditional approach:**\n",
        "- We treat each Sentinel-2 scene as a complete \"file\" \n",
        "- We load the full scene into memory\n",
        "- We process it entirely before moving to the next scene\n",
        "\n",
        "This is in contrast to modern **tile-based** approaches where only small chunks of data are loaded as needed.\n",
        "\n",
        "### What happens in this section?\n",
        "\n",
        "1. **Load a scene** → Download the satellite image bands we need (Green and SWIR channels)\n",
        "2. **Calculate snow index** → Apply the NDSI formula to identify snow/ice pixels\n",
        "3. **Create a mask** → Mark which pixels are snow and which are not\n",
        "\n",
        "### Why doesn't it work?\n",
        "\n",
        "The Sentinel-2 scenes from the EOPF store are very large (hundreds of MB to GB per scene). When we try to load an entire scene into memory at once, JupyterHub runs out of RAM and crashes, even with just one scene.\n",
        "\n",
        "> **Bottom line:** This approach demonstrates the algorithm logic, but cannot actually run in resource-limited environments like our JupyterHub."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_scene(href, x_slice, y_slice):\n",
        "    \"\"\"\n",
        "    Load a single Sentinel-2 scene from EOPF Zarr store.\n",
        "    Returns bands B03 (Green), B11 (SWIR), and SCL mask aligned to 10m grid.\n",
        "    \"\"\"\n",
        "    scene_name = os.path.basename(href.rstrip(\"/\"))\n",
        "    print(f\"Loading scene: {scene_name}\")\n",
        "    \n",
        "    # Load Green Band (B03) - 10m resolution\n",
        "    ds_b03 = xr.open_zarr(href, group=\"/measurements/reflectance/r10m\")[[\"b03\"]]\n",
        "    ds_b03 = ds_b03.sel(x=x_slice, y=y_slice)\n",
        "    \n",
        "    # Load SWIR Band (B11) - 20m resolution\n",
        "    ds_b11 = xr.open_zarr(href, group=\"/measurements/reflectance/r20m\")[[\"b11\"]]\n",
        "    ds_b11 = ds_b11.sel(x=x_slice, y=y_slice)\n",
        "    \n",
        "    # Load SCL (Scene Classification) - 20m resolution\n",
        "    ds_scl = xr.open_zarr(href, group=\"/conditions/mask/l2a_classification/r20m\")[[\"scl\"]]\n",
        "    ds_scl = ds_scl.sel(x=x_slice, y=y_slice)\n",
        "    \n",
        "    # Resample 20m bands to 10m grid\n",
        "    ds_b11_interp = ds_b11.interp_like(ds_b03, method=\"nearest\")\n",
        "    ds_scl_interp = ds_scl.interp_like(ds_b03, method=\"nearest\")\n",
        "    \n",
        "    # Merge into single dataset\n",
        "    scene_data = xr.merge([ds_b03, ds_b11_interp, ds_scl_interp])\n",
        "    \n",
        "    # Extract datetime from filename\n",
        "    parts = scene_name.split(\"_\")\n",
        "    date_str = next((p for p in parts if p.startswith(\"20\") and \"T\" in p), None)\n",
        "    if date_str:\n",
        "        scene_data.attrs[\"datetime\"] = datetime.strptime(date_str.split(\".\")[0], \"%Y%m%dT%H%M%S\")\n",
        "    \n",
        "    scene_data.attrs[\"scene_name\"] = scene_name\n",
        "    return scene_data\n",
        "\n",
        "\n",
        "def compute_ndsi(scene_data):\n",
        "    \"\"\"\n",
        "    Compute NDSI (Normalized Difference Snow Index) and snow mask.\n",
        "    NDSI = (Green - SWIR) / (Green + SWIR)\n",
        "    Snow threshold: NDSI > 0.42\n",
        "    \"\"\"\n",
        "    green = scene_data[\"b03\"]\n",
        "    swir = scene_data[\"b11\"]\n",
        "    scl = scene_data[\"scl\"]\n",
        "    \n",
        "    # Calculate NDSI\n",
        "    denom = green + swir\n",
        "    ndsi = (green - swir) / denom.where(denom != 0)\n",
        "    \n",
        "    # Mask: exclude Nodata (0), Saturated (1), Cloud Shadows (3)[it can see snow even if there are cloud shadows], Water (6), Unclassified (7), Cloud Medium (8), Cloud High (9)\n",
        "    valid_mask = ~scl.isin([0,1,6,7,8,9])\n",
        "    \n",
        "    # Snow classification (NDSI > 0.42 and valid pixel)\n",
        "    snow_mask = (ndsi > 0.42) & valid_mask\n",
        "    \n",
        "    return xr.Dataset({\n",
        "        \"ndsi\": ndsi,\n",
        "        \"snow_mask\": snow_mask,\n",
        "        \"valid_mask\": valid_mask,\n",
        "    }, attrs=scene_data.attrs)\n",
        "\n",
        "\n",
        "def process_scenes(hrefs, x_slice, y_slice):\n",
        "    \"\"\"\n",
        "    Process all scenes in file-based workflow.\n",
        "    Returns list of results (one per scene).\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    \n",
        "    for i, href in enumerate(hrefs):\n",
        "        print(f\"\\n[{i+1}/{len(hrefs)}] Processing...\")\n",
        "        try:\n",
        "            # Load scene\n",
        "            scene_data = load_scene(href, x_slice, y_slice)\n",
        "            \n",
        "            # Compute NDSI and snow mask\n",
        "            result = compute_ndsi(scene_data)\n",
        "            \n",
        "            # Trigger computation\n",
        "            result = result.compute()\n",
        "            \n",
        "            # Statistics\n",
        "            snow_pixels = result[\"snow_mask\"].sum().item()\n",
        "            valid_pixels = result[\"valid_mask\"].sum().item()\n",
        "            snow_percent = (snow_pixels / valid_pixels * 100) if valid_pixels > 0 else 0\n",
        "            \n",
        "            print(f\"  → Snow pixels: {snow_pixels:,} ({snow_percent:.1f}% of valid area)\")\n",
        "            \n",
        "            results.append(result)\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"  → Error: {e}\")\n",
        "            continue\n",
        "    \n",
        "    return results\n",
        "\n",
        "\n",
        "# Process scenes\n",
        "if hrefs:\n",
        "    print(f\"Starting file-based processing of {len(hrefs)} scene(s)...\\n\")\n",
        "    results = process_scenes(hrefs, x_slice, y_slice)\n",
        "    print(f\"\\n✓ Successfully processed {len(results)} scene(s)\")\n",
        "else:\n",
        "    print(\"No scenes to process.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualization: Stacked Scene Composite\n",
        "\n",
        "This section creates a composite from multiple processed scenes.\n",
        "\n",
        "**What this code does:**\n",
        "- Stacks scenes and computes the **median NDSI** (robust against outliers and clouds)\n",
        "- Builds a **valid mask** (pixel valid in at least one scene)\n",
        "- Applies the valid mask to the median NDSI for cleaner visualization\n",
        "- Creates an aggregated snow mask based on the median NDSI threshold\n",
        "- Calculates snow coverage statistics\n",
        "\n",
        "**Why use median?**\n",
        "Using the median across multiple scenes helps to:\n",
        "- Reduce noise from clouds that weren't fully masked\n",
        "- Create a more stable snow/ice classification\n",
        "- Fill gaps where individual scenes had invalid data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ======================================================================\n",
        "# Stack Results and Compute Composite\n",
        "# ======================================================================\n",
        "# Combines all processed scenes into a single composite using median/mean\n",
        "\n",
        "NDSI_THRESHOLD = 0.42  # Standard snow threshold\n",
        "\n",
        "def create_stacked_composite(results):\n",
        "    \"\"\"\n",
        "    Stack all scene results and compute composite metrics.\n",
        "    - Median NDSI (robust against outliers/clouds)\n",
        "    - Aggregated snow mask (snow if detected in majority of valid scenes)\n",
        "    \"\"\"\n",
        "    if not results:\n",
        "        return None\n",
        "    \n",
        "    scene_count = len(results)\n",
        "    print(f\"Stacking {scene_count} scene(s)...\")\n",
        "    \n",
        "    if scene_count == 1:\n",
        "        # Single scene: use directly\n",
        "        ndsi_median = results[0][\"ndsi\"]\n",
        "        snow_mask = results[0][\"snow_mask\"]\n",
        "        valid_mask = results[0][\"valid_mask\"]\n",
        "    else:\n",
        "        # Multiple scenes: stack along new dimension and compute median\n",
        "        ndsi_stack = xr.concat([r[\"ndsi\"] for r in results], dim=\"scene\")\n",
        "        valid_stack = xr.concat([r[\"valid_mask\"] for r in results], dim=\"scene\")\n",
        "        \n",
        "        # Median NDSI (ignoring NaN)\n",
        "        ndsi_median = ndsi_stack.median(dim=\"scene\", skipna=True)\n",
        "        \n",
        "        # Valid mask: pixel valid in at least one scene\n",
        "        valid_mask = valid_stack.any(dim=\"scene\")\n",
        "        \n",
        "        # Snow mask from median NDSI\n",
        "        snow_mask = (ndsi_median > NDSI_THRESHOLD) & valid_mask\n",
        "    \n",
        "    # Apply valid mask to NDSI for clean visualization\n",
        "    ndsi_median = ndsi_median.where(valid_mask)\n",
        "    \n",
        "    # Calculate statistics\n",
        "    total_pixels = valid_mask.size\n",
        "    valid_pixels = valid_mask.sum().item()\n",
        "    snow_pixels = snow_mask.sum().item()\n",
        "    snow_percentage = (snow_pixels / valid_pixels * 100) if valid_pixels > 0 else 0\n",
        "    \n",
        "    print(f\"  → Total pixels: {total_pixels:,}\")\n",
        "    print(f\"  → Valid pixels: {valid_pixels:,}\")\n",
        "    print(f\"  → Snow pixels: {snow_pixels:,} ({snow_percentage:.1f}%)\")\n",
        "    \n",
        "    return {\n",
        "        \"ndsi_median\": ndsi_median,\n",
        "        \"snow_mask\": snow_mask,\n",
        "        \"valid_mask\": valid_mask,\n",
        "        \"scene_count\": scene_count,\n",
        "        \"snow_percentage\": snow_percentage\n",
        "    }\n",
        "\n",
        "\n",
        "# Create composite from results\n",
        "if 'results' in dir() and results:\n",
        "    composite = create_stacked_composite(results)\n",
        "else:\n",
        "    composite = None\n",
        "    print(\"No results available. Run processing cell first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### NDSI Visualization\n",
        "\n",
        "Creates a 2-panel diagnostic plot:\n",
        "1. **Median NDSI Map** - Shows the spectral snow index values (-0.5 to 1.0). Green indicates snow/ice (high NDSI), red indicates bare ground/water (low NDSI).\n",
        "2. **Binary Snow/Ice Mask** - Shows classified snow pixels where NDSI ≥ 0.42."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ======================================================================\n",
        "# Visualization: NDSI and Snow/Ice Classification Results\n",
        "# ======================================================================\n",
        "# Creates a 2-panel diagnostic plot:\n",
        "# 1. Median NDSI map (spectral snow index)\n",
        "# 2. Binary snow/ice mask (pixels where NDSI >= threshold)\n",
        "\n",
        "if composite:\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "    \n",
        "    # ----- Plot 1: Median NDSI -----\n",
        "    ndsi_data = composite['ndsi_median'].values\n",
        "    im1 = axes[0].imshow(ndsi_data, cmap='RdYlGn', vmin=-0.5, vmax=1.0)\n",
        "    axes[0].set_title(f'Median NDSI\\n(from {composite[\"scene_count\"]} scene(s))', \n",
        "                      fontsize=12, fontweight='bold')\n",
        "    axes[0].set_xlabel('Pixel X')\n",
        "    axes[0].set_ylabel('Pixel Y')\n",
        "    plt.colorbar(im1, ax=axes[0], label='NDSI', shrink=0.8)\n",
        "    \n",
        "    # ----- Plot 2: Snow/Ice Mask -----\n",
        "    snow_data = composite['snow_mask'].values.astype(float)\n",
        "    im2 = axes[1].imshow(snow_data, cmap='Blues', vmin=0, vmax=1)\n",
        "    axes[1].set_title(f'Snow/Ice Mask\\n(NDSI >= {NDSI_THRESHOLD})', \n",
        "                      fontsize=12, fontweight='bold')\n",
        "    axes[1].set_xlabel('Pixel X')\n",
        "    axes[1].set_ylabel('Pixel Y')\n",
        "    \n",
        "    # Add percentage annotation\n",
        "    axes[1].text(0.5, 0.95, f'{composite[\"snow_percentage\"]:.1f}% Snow Coverage', \n",
        "                 transform=axes[1].transAxes, ha='center', va='top', fontsize=11, \n",
        "                 bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(f\"\\n✓ Visualization complete\")\n",
        "    print(f\"  Scenes processed: {composite['scene_count']}\")\n",
        "    print(f\"  Snow coverage: {composite['snow_percentage']:.1f}%\")\n",
        "else:\n",
        "    print(\"No composite available. Run stacking cell first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## RGB Visualization\n",
        "\n",
        "This section loads RGB bands **independently** from the NDSI processing to avoid data interference.\n",
        "\n",
        "**Why separate processing?**\n",
        "- Keeps the NDSI data pipeline clean and unmodified\n",
        "- RGB is only for visualization purposes\n",
        "- Uses 60m resolution bands to minimize memory usage\n",
        "\n",
        "**What this code does:**\n",
        "- Loads B02 (Blue), B03 (Green), B04 (Red) at 60m resolution\n",
        "- Builds a median RGB composite **without cloud masking** to keep full visual context\n",
        "- Applies percentile stretching for better visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ======================================================================\n",
        "# RGB Processing - Separate from NDSI\n",
        "# ======================================================================\n",
        "# Loads RGB bands at 60m resolution to minimize memory/container load\n",
        "\n",
        "def load_rgb_scene(href, x_slice, y_slice):\n",
        "    \"\"\"\n",
        "    Load RGB bands (B02, B03, B04) at 60m resolution for a single scene.\n",
        "    Uses 60m bands for minimal data volume and container load.\n",
        "    \"\"\"\n",
        "    scene_name = os.path.basename(href.rstrip(\"/\"))\n",
        "    \n",
        "    # Load 60m RGB bands (lowest resolution = minimal data)\n",
        "    ds_rgb = xr.open_zarr(href, group=\"/measurements/reflectance/r60m\")[[\"b02\", \"b03\", \"b04\"]]\n",
        "    ds_rgb = ds_rgb.sel(x=x_slice, y=y_slice)\n",
        "    \n",
        "    return ds_rgb\n",
        "\n",
        "\n",
        "def create_rgb_composite(hrefs, x_slice, y_slice):\n",
        "    \"\"\"\n",
        "    Create median RGB composite from all scenes.\n",
        "    No cloud mask is applied to keep true visual context.\n",
        "    \"\"\"\n",
        "    print(f\"Loading RGB data from {len(hrefs)} scene(s)...\")\n",
        "    \n",
        "    rgb_scenes = []\n",
        "    for i, href in enumerate(hrefs):\n",
        "        try:\n",
        "            scene = load_rgb_scene(href, x_slice, y_slice)\n",
        "            rgb_scenes.append(scene.compute())\n",
        "            print(f\"  [{i+1}/{len(hrefs)}] Loaded: {os.path.basename(href)}\")\n",
        "        except Exception as e:\n",
        "            print(f\"  [{i+1}/{len(hrefs)}] Error: {e}\")\n",
        "            continue\n",
        "    \n",
        "    if not rgb_scenes:\n",
        "        return None\n",
        "    \n",
        "    if len(rgb_scenes) == 1:\n",
        "        # Single scene\n",
        "        rgb_median = xr.concat([\n",
        "            rgb_scenes[0][\"b04\"],\n",
        "            rgb_scenes[0][\"b03\"],\n",
        "            rgb_scenes[0][\"b02\"],\n",
        "        ], dim=\"band\").assign_coords(band=[\"R\", \"G\", \"B\"])\n",
        "    else:\n",
        "        # Stack and compute median\n",
        "        r_stack = xr.concat([s[\"b04\"] for s in rgb_scenes], dim=\"scene\")\n",
        "        g_stack = xr.concat([s[\"b03\"] for s in rgb_scenes], dim=\"scene\")\n",
        "        b_stack = xr.concat([s[\"b02\"] for s in rgb_scenes], dim=\"scene\")\n",
        "        \n",
        "        rgb_median = xr.concat([\n",
        "            r_stack.median(dim=\"scene\", skipna=True),\n",
        "            g_stack.median(dim=\"scene\", skipna=True),\n",
        "            b_stack.median(dim=\"scene\", skipna=True),\n",
        "        ], dim=\"band\").assign_coords(band=[\"R\", \"G\", \"B\"])\n",
        "    \n",
        "    print(f\"✓ RGB composite created from {len(rgb_scenes)} scene(s)\")\n",
        "    return {\"rgb_median\": rgb_median, \"scene_count\": len(rgb_scenes)}\n",
        "\n",
        "\n",
        "# Create RGB composite (independent from NDSI)\n",
        "if hrefs:\n",
        "    rgb_composite = create_rgb_composite(hrefs, x_slice, y_slice)\n",
        "else:\n",
        "    rgb_composite = None\n",
        "    print(\"No scenes available for RGB processing.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### RGB vs Snow Mask Comparison\n",
        "\n",
        "Creates a side-by-side comparison:\n",
        "1. **Median RGB Composite** - True color visualization of the scene(s) with 2-98% percentile stretch\n",
        "2. **Snow/Ice Mask** - Binary classification overlaid for comparison with the visual appearance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ======================================================================\n",
        "# Visualization: Median RGB vs Snow/Ice Mask\n",
        "# ======================================================================\n",
        "# Uses independently loaded RGB data\n",
        "\n",
        "if rgb_composite and composite:\n",
        "    # Prepare RGB composite (R, G, B)\n",
        "    rgb_da = rgb_composite['rgb_median']\n",
        "    r = rgb_da.sel(band=\"R\").values\n",
        "    g = rgb_da.sel(band=\"G\").values\n",
        "    b = rgb_da.sel(band=\"B\").values\n",
        "    \n",
        "    # Stack and handle NaNs\n",
        "    rgb_np = np.stack([r, g, b], axis=-1).astype(\"float32\")\n",
        "    rgb_np = np.nan_to_num(rgb_np, nan=0.0)\n",
        "    \n",
        "    # If reflectance is scaled (e.g., 0-10000), normalize to 0-1\n",
        "    max_val = np.nanmax(rgb_np)\n",
        "    if max_val > 1.5:\n",
        "        rgb_np = rgb_np / 10000.0\n",
        "    \n",
        "    # Per-channel percentile stretch (better color balance)\n",
        "    rgb_stretched = np.zeros_like(rgb_np)\n",
        "    for c in range(3):\n",
        "        channel = rgb_np[..., c]\n",
        "        valid = np.isfinite(channel) & (channel > 0)\n",
        "        if np.any(valid):\n",
        "            low, high = np.percentile(channel[valid], [2, 98])\n",
        "            channel = (channel - low) / (high - low + 1e-6)\n",
        "            channel = np.clip(channel, 0, 1)\n",
        "        rgb_stretched[..., c] = channel\n",
        "    \n",
        "    # Optional gamma correction to reduce bright clouds dominance\n",
        "    gamma = 1.2\n",
        "    rgb_stretched = np.power(rgb_stretched, 1 / gamma)\n",
        "    \n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "    \n",
        "    # ----- Plot 1: RGB Composite -----\n",
        "    axes[0].imshow(rgb_stretched)\n",
        "    axes[0].set_title(f\"Median RGB Composite\\n(from {rgb_composite['scene_count']} scene(s))\", \n",
        "                     fontsize=12, fontweight='bold')\n",
        "    axes[0].axis('off')\n",
        "    \n",
        "    # ----- Plot 2: Snow/Ice Mask -----\n",
        "    snow_data = composite['snow_mask'].values.astype(float)\n",
        "    axes[1].imshow(snow_data, cmap='Blues', vmin=0, vmax=1)\n",
        "    axes[1].set_title(f\"Snow/Ice Mask\\n(NDSI >= {NDSI_THRESHOLD})\", \n",
        "                     fontsize=12, fontweight='bold')\n",
        "    axes[1].axis('off')\n",
        "    \n",
        "    # Add percentage annotation\n",
        "    axes[1].text(0.5, -0.05, f\"{composite['snow_percentage']:.1f}% Snow Coverage\",\n",
        "                 transform=axes[1].transAxes, ha='center', fontsize=11,\n",
        "                 bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"RGB or NDSI composite missing. Run processing cells first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Interactive Map: Scene Locations & Processed Area\n",
        "\n",
        "This section creates an interactive map showing the spatial context of the analysis.\n",
        "\n",
        "**Map Layers:**\n",
        "- **Green polygon**: The actual processed composite area (data extent after spatial slicing)\n",
        "- **Blue polygons**: Full Sentinel-2 scene extents from the STAC catalog\n",
        "- **Red points**: Glacier seed locations used for the AOI definition\n",
        "- **Orange dashed box**: Area of Interest (AOI) bounding box\n",
        "\n",
        "**Features:**\n",
        "- Esri satellite imagery basemap\n",
        "- Layer control to toggle visibility of each layer\n",
        "- Hover over polygons for scene details (ID, date, cloud cover)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ======================================================================\n",
        "# Interactive Map with Folium\n",
        "# ======================================================================\n",
        "# Shows scene bounding boxes on a satellite background\n",
        "# Highlights the actual processed area (composite bbox) in green\n",
        "\n",
        "import folium\n",
        "from folium import GeoJson, FeatureGroup\n",
        "from folium.plugins import Fullscreen\n",
        "\n",
        "# Create GeoDataFrame with all scene bounding boxes (blue)\n",
        "scene_bboxes = []\n",
        "for item in valid_items:\n",
        "    bbox = item.bbox  # [west, south, east, north]\n",
        "    scene_polygon = box(bbox[0], bbox[1], bbox[2], bbox[3])\n",
        "    scene_bboxes.append({\n",
        "        \"geometry\": scene_polygon,\n",
        "        \"scene_id\": item.id,\n",
        "        \"datetime\": item.datetime.strftime(\"%Y-%m-%d %H:%M\") if item.datetime else \"N/A\",\n",
        "        \"cloud_cover\": item.properties.get(\"eo:cloud_cover\", \"N/A\")\n",
        "    })\n",
        "\n",
        "scenes_gdf = gpd.GeoDataFrame(scene_bboxes, crs=\"EPSG:4326\")\n",
        "\n",
        "# === Get Composite BBox (actual processed area) ===\n",
        "composite_gdf = None\n",
        "if 'composite' in dir() and composite is not None:\n",
        "    # Extract coordinates from composite (in UTM)\n",
        "    try:\n",
        "        x_coords = composite['ndsi_median'].coords['x'].values\n",
        "        y_coords = composite['ndsi_median'].coords['y'].values\n",
        "        \n",
        "        # Get bounds in UTM\n",
        "        utm_west = float(x_coords.min())\n",
        "        utm_east = float(x_coords.max())\n",
        "        utm_south = float(y_coords.min())\n",
        "        utm_north = float(y_coords.max())\n",
        "        \n",
        "        # Transform back to WGS84\n",
        "        transformer_to_wgs84 = Transformer.from_crs(target_crs, \"EPSG:4326\", always_xy=True)\n",
        "        \n",
        "        # Transform all 4 corners to handle projection distortion\n",
        "        sw_lon, sw_lat = transformer_to_wgs84.transform(utm_west, utm_south)\n",
        "        se_lon, se_lat = transformer_to_wgs84.transform(utm_east, utm_south)\n",
        "        ne_lon, ne_lat = transformer_to_wgs84.transform(utm_east, utm_north)\n",
        "        nw_lon, nw_lat = transformer_to_wgs84.transform(utm_west, utm_north)\n",
        "        \n",
        "        # Create polygon from corners\n",
        "        from shapely.geometry import Polygon\n",
        "        composite_polygon = Polygon([\n",
        "            (sw_lon, sw_lat),\n",
        "            (se_lon, se_lat),\n",
        "            (ne_lon, ne_lat),\n",
        "            (nw_lon, nw_lat),\n",
        "            (sw_lon, sw_lat)  # close the ring\n",
        "        ])\n",
        "        \n",
        "        composite_gdf = gpd.GeoDataFrame([{\n",
        "            \"geometry\": composite_polygon,\n",
        "            \"name\": \"Processed Composite Area\",\n",
        "            \"scene_count\": composite.get(\"scene_count\", 1),\n",
        "            \"snow_coverage\": f\"{composite.get('snow_percentage', 0):.1f}%\"\n",
        "        }], crs=\"EPSG:4326\")\n",
        "        \n",
        "        print(f\"Composite bbox (UTM): W={utm_west:.0f}, E={utm_east:.0f}, S={utm_south:.0f}, N={utm_north:.0f}\")\n",
        "        print(f\"Composite bbox (WGS84): W={sw_lon:.4f}, E={ne_lon:.4f}, S={sw_lat:.4f}, N={ne_lat:.4f}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Could not extract composite bbox: {e}\")\n",
        "\n",
        "# Calculate map center\n",
        "if composite_gdf is not None:\n",
        "    minx, miny, maxx, maxy = composite_gdf.total_bounds\n",
        "    center_lon = (minx + maxx) / 2\n",
        "    center_lat = (miny + maxy) / 2\n",
        "else:\n",
        "    center_lon = (bbox_4326[0] + bbox_4326[2]) / 2\n",
        "    center_lat = (bbox_4326[1] + bbox_4326[3]) / 2\n",
        "\n",
        "# Create map with Esri satellite tiles\n",
        "m = folium.Map(\n",
        "    location=[center_lat, center_lon],\n",
        "    zoom_start=7,\n",
        "    tiles=None\n",
        ")\n",
        "\n",
        "# Add Esri World Imagery as satellite basemap\n",
        "folium.TileLayer(\n",
        "    tiles=\"https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}\",\n",
        "    attr=\"Esri, Maxar, Earthstar Geographics\",\n",
        "    name=\"Satellite\",\n",
        "    overlay=False\n",
        ").add_to(m)\n",
        "\n",
        "# === Layer 1: All Sentinel-2 Scenes (Blue) ===\n",
        "scenes_layer = FeatureGroup(name=\"All Sentinel-2 Scenes\")\n",
        "GeoJson(\n",
        "    scenes_gdf.__geo_interface__,\n",
        "    style_function=lambda x: {\n",
        "        \"fillColor\": \"#3388ff\",\n",
        "        \"color\": \"#3388ff\",\n",
        "        \"weight\": 2,\n",
        "        \"fillOpacity\": 0.15\n",
        "    },\n",
        "    tooltip=folium.GeoJsonTooltip(\n",
        "        fields=[\"scene_id\", \"datetime\", \"cloud_cover\"],\n",
        "        aliases=[\"Scene ID:\", \"Date:\", \"Cloud Cover:\"],\n",
        "        localize=True\n",
        "    )\n",
        ").add_to(scenes_layer)\n",
        "scenes_layer.add_to(m)\n",
        "\n",
        "# === Layer 2: Composite Processed Area (Green) ===\n",
        "if composite_gdf is not None:\n",
        "    composite_layer = FeatureGroup(name=\"PROCESSED AREA (Composite)\")\n",
        "    GeoJson(\n",
        "        composite_gdf.__geo_interface__,\n",
        "        style_function=lambda x: {\n",
        "            \"fillColor\": \"#00ff00\",\n",
        "            \"color\": \"#00ff00\",\n",
        "            \"weight\": 5,\n",
        "            \"fillOpacity\": 0.25\n",
        "        },\n",
        "        tooltip=folium.GeoJsonTooltip(\n",
        "            fields=[\"name\", \"scene_count\", \"snow_coverage\"],\n",
        "            aliases=[\"Area:\", \"Scenes:\", \"Snow:\"],\n",
        "            localize=True\n",
        "        )\n",
        "    ).add_to(composite_layer)\n",
        "    composite_layer.add_to(m)\n",
        "\n",
        "# === Layer 3: Seed Points (Red) ===\n",
        "seeds_layer = FeatureGroup(name=\"Glacier Seed Points\")\n",
        "for idx, row in seeds_gdf.iterrows():\n",
        "    folium.CircleMarker(\n",
        "        location=[row.geometry.y, row.geometry.x],\n",
        "        radius=5,\n",
        "        color=\"#ffffff\",\n",
        "        fill=True,\n",
        "        fillColor=\"#ff0000\",\n",
        "        fillOpacity=1.0,\n",
        "        weight=1\n",
        "    ).add_to(seeds_layer)\n",
        "seeds_layer.add_to(m)\n",
        "\n",
        "# === Layer 4: AOI Bounding Box (Orange) ===\n",
        "aoi_polygon = box(bbox_4326[0], bbox_4326[1], bbox_4326[2], bbox_4326[3])\n",
        "aoi_gdf = gpd.GeoDataFrame([{\"geometry\": aoi_polygon, \"name\": \"AOI\"}], crs=\"EPSG:4326\")\n",
        "\n",
        "aoi_layer = FeatureGroup(name=\"Area of Interest (AOI)\")\n",
        "GeoJson(\n",
        "    aoi_gdf.__geo_interface__,\n",
        "    style_function=lambda x: {\n",
        "        \"fillColor\": \"transparent\",\n",
        "        \"color\": \"#ff6600\",\n",
        "        \"weight\": 3,\n",
        "        \"dashArray\": \"10, 5\"\n",
        "    }\n",
        ").add_to(aoi_layer)\n",
        "aoi_layer.add_to(m)\n",
        "\n",
        "# Add layer control and fullscreen button\n",
        "folium.LayerControl(collapsed=False).add_to(m)\n",
        "Fullscreen().add_to(m)\n",
        "\n",
        "# Print legend\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(f\"Map Legend:\")\n",
        "if composite_gdf is not None:\n",
        "    print(f\"   Green polygon: Actual processed composite area\")\n",
        "    print(f\"    (This is the data extent after spatial slicing)\")\n",
        "print(f\"  • Blue polygons: Full Sentinel-2 scene extents ({len(scenes_gdf)})\")\n",
        "print(f\"  • Red points: Glacier seed locations ({len(seeds_gdf)})\")\n",
        "print(f\"  • Orange dashed: Area of Interest (AOI)\")\n",
        "print(f\"{'='*50}\")\n",
        "\n",
        "m"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
