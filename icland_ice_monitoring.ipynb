{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Iceland Snow and Ice Monitoring\n",
        "\n",
        "This notebook implements a workflow for monitoring snow and ice in Iceland using Sentinel-2 data via the EOPF Zarr format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import geopandas as gpd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "from matplotlib.patches import Patch\n",
        "from shapely.geometry import box\n",
        "import shapely.geometry\n",
        "import pystac_client\n",
        "from pystac import Item\n",
        "import xarray as xr\n",
        "import os\n",
        "import requests\n",
        "from datetime import datetime\n",
        "from pyproj import Transformer\n",
        "import dask\n",
        "import warnings\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Seeds\n",
        "Load the glacier seeds (points) and define the Area of Interest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load seeds\n",
        "seeds_gdf = gpd.read_file(\"data/Iceland_Seeds.geojson\")\n",
        "\n",
        "# Reproject to WGS84 for search\n",
        "seeds_gdf = seeds_gdf.to_crs(\"EPSG:4326\")\n",
        "\n",
        "# Get bounding box in WGS84\n",
        "total_bounds = seeds_gdf.total_bounds\n",
        "bbox_4326 = list(total_bounds) # [minx, miny, maxx, maxy]\n",
        "\n",
        "# Define AOI for UTM transformation\n",
        "spatial_extent = {\n",
        "    \"west\": bbox_4326[0],\n",
        "    \"south\": bbox_4326[1],\n",
        "    \"east\": bbox_4326[2],\n",
        "    \"north\": bbox_4326[3],\n",
        "}\n",
        "\n",
        "print(f\"Bbox (EPSG:4326): {bbox_4326}\")\n",
        "\n",
        "# Convert AOI to UTM 27N (EPSG:32627) - Common for Iceland\n",
        "# The example used EPSG:32631 for Belgium. For Iceland, we use 32627.\n",
        "target_crs = \"EPSG:32627\"\n",
        "transformer = Transformer.from_crs(\"EPSG:4326\", target_crs, always_xy=True)\n",
        "\n",
        "west_utm, south_utm = transformer.transform(\n",
        "    spatial_extent[\"west\"], spatial_extent[\"south\"]\n",
        ")\n",
        "east_utm, north_utm = transformer.transform(\n",
        "    spatial_extent[\"east\"], spatial_extent[\"north\"]\n",
        ")\n",
        "\n",
        "# Spatial slice parameters (Note: y is typically north-to-south in these grids, so slice order matters)\n",
        "# We will verify the order after inspection, but typically it is slice(max_y, min_y) or slice(min_y, max_y) depending on the index.\n",
        "# The example used slice(north_utm, south_utm) for y, implying descending coordinates.\n",
        "x_slice = slice(west_utm, east_utm)\n",
        "y_slice = slice(north_utm, south_utm)\n",
        "\n",
        "print(f\"UTM Bounds ({target_crs}): West={west_utm}, South={south_utm}, East={east_utm}, North={north_utm}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## STAC Search and Data Loading\n",
        "\n",
        "This algorithm simulates a file-based workflow by processing full Sentinel-2 scenes (Sentinel-2 L2A) retrieved from the EOPF Zarr store. \n",
        "It avoids tile-based optimization and instead loads the full scene extent to compute NDSI and classify snow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Configuration ===\n",
        "SINGLE_SCENE_MODE = True  # Toggle: True = only first scene, False = all scenes\n",
        "\n",
        "# STAC Search\n",
        "catalog = pystac_client.Client.open(\"https://stac.core.eopf.eodc.eu\")\n",
        "\n",
        "# Search for Sentinel-2 L2A items\n",
        "time_range_str = \"2025-09-01/2025-09-03\"\n",
        "\n",
        "print(f\"Searching STAC for {time_range_str} over AOI...\")\n",
        "search = catalog.search(\n",
        "    collections=[\"sentinel-2-l2a\"],\n",
        "    bbox=bbox_4326,\n",
        "    datetime=time_range_str,\n",
        ")\n",
        "\n",
        "items = list(search.items())\n",
        "print(f\"Found {len(items)} items.\")\n",
        "\n",
        "# Filter: only non-deprecated items with 'product' asset\n",
        "valid_items = [\n",
        "    item for item in items \n",
        "    if not item.properties.get(\"deprecated\", False) and \"product\" in item.assets\n",
        "]\n",
        "print(f\"Valid items with product asset: {len(valid_items)}\")\n",
        "\n",
        "# Get hrefs for file-based processing\n",
        "hrefs = [item.assets[\"product\"].href for item in valid_items]\n",
        "\n",
        "# Apply single scene toggle\n",
        "if SINGLE_SCENE_MODE and hrefs:\n",
        "    hrefs = [hrefs[0]]\n",
        "    print(f\"\\n[SINGLE_SCENE_MODE] Using only first scene: {os.path.basename(hrefs[0])}\")\n",
        "elif hrefs:\n",
        "    print(f\"\\nTotal scenes to process: {len(hrefs)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## File-Based Scene Processing\n",
        "\n",
        "Load and process each Sentinel-2 scene individually. This simulates a file-based workflow where each scene is processed one at a time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_scene(href, x_slice, y_slice):\n",
        "    \"\"\"\n",
        "    Load a single Sentinel-2 scene from EOPF Zarr store.\n",
        "    Returns bands B03 (Green), B11 (SWIR), and SCL mask aligned to 10m grid.\n",
        "    \"\"\"\n",
        "    scene_name = os.path.basename(href.rstrip(\"/\"))\n",
        "    print(f\"Loading scene: {scene_name}\")\n",
        "    \n",
        "    # Load Green Band (B03) - 10m resolution\n",
        "    ds_b03 = xr.open_zarr(href, group=\"/measurements/reflectance/r10m\")[[\"b03\"]]\n",
        "    ds_b03 = ds_b03.sel(x=x_slice, y=y_slice)\n",
        "    \n",
        "    # Load SWIR Band (B11) - 20m resolution\n",
        "    ds_b11 = xr.open_zarr(href, group=\"/measurements/reflectance/r20m\")[[\"b11\"]]\n",
        "    ds_b11 = ds_b11.sel(x=x_slice, y=y_slice)\n",
        "    \n",
        "    # Load SCL (Scene Classification) - 20m resolution\n",
        "    ds_scl = xr.open_zarr(href, group=\"/conditions/mask/l2a_classification/r20m\")[[\"scl\"]]\n",
        "    ds_scl = ds_scl.sel(x=x_slice, y=y_slice)\n",
        "    \n",
        "    # Resample 20m bands to 10m grid (align to B03)\n",
        "    ds_b11_interp = ds_b11.interp_like(ds_b03, method=\"nearest\")\n",
        "    ds_scl_interp = ds_scl.interp_like(ds_b03, method=\"nearest\")\n",
        "    \n",
        "    # Merge into single dataset\n",
        "    scene_data = xr.merge([ds_b03, ds_b11_interp, ds_scl_interp])\n",
        "    \n",
        "    # Extract datetime from filename\n",
        "    parts = scene_name.split(\"_\")\n",
        "    date_str = next((p for p in parts if p.startswith(\"20\") and \"T\" in p), None)\n",
        "    if date_str:\n",
        "        scene_data.attrs[\"datetime\"] = datetime.strptime(date_str.split(\".\")[0], \"%Y%m%dT%H%M%S\")\n",
        "    \n",
        "    scene_data.attrs[\"scene_name\"] = scene_name\n",
        "    return scene_data\n",
        "\n",
        "\n",
        "def compute_ndsi(scene_data):\n",
        "    \"\"\"\n",
        "    Compute NDSI (Normalized Difference Snow Index) and snow mask.\n",
        "    NDSI = (Green - SWIR) / (Green + SWIR)\n",
        "    Snow threshold: NDSI > 0.42\n",
        "    \"\"\"\n",
        "    green = scene_data[\"b03\"]\n",
        "    swir = scene_data[\"b11\"]\n",
        "    scl = scene_data[\"scl\"]\n",
        "    \n",
        "    # Calculate NDSI\n",
        "    denom = green + swir\n",
        "    ndsi = (green - swir) / denom.where(denom != 0)\n",
        "    \n",
        "    # Cloud mask: exclude Cloud Shadows (3), Cloud Medium (8), Cloud High (9)\n",
        "    valid_mask = ~scl.isin([3, 8, 9])\n",
        "    \n",
        "    # Snow classification (NDSI > 0.42 and valid pixel)\n",
        "    snow_mask = (ndsi > 0.42) & valid_mask\n",
        "    \n",
        "    return xr.Dataset({\n",
        "        \"ndsi\": ndsi,\n",
        "        \"snow_mask\": snow_mask,\n",
        "        \"valid_mask\": valid_mask\n",
        "    }, attrs=scene_data.attrs)\n",
        "\n",
        "\n",
        "def process_scenes(hrefs, x_slice, y_slice):\n",
        "    \"\"\"\n",
        "    Process all scenes in file-based workflow.\n",
        "    Returns list of results (one per scene).\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    \n",
        "    for i, href in enumerate(hrefs):\n",
        "        print(f\"\\n[{i+1}/{len(hrefs)}] Processing...\")\n",
        "        try:\n",
        "            # Load scene\n",
        "            scene_data = load_scene(href, x_slice, y_slice)\n",
        "            \n",
        "            # Compute NDSI and snow mask\n",
        "            result = compute_ndsi(scene_data)\n",
        "            \n",
        "            # Trigger computation\n",
        "            result = result.compute()\n",
        "            \n",
        "            # Statistics\n",
        "            snow_pixels = result[\"snow_mask\"].sum().item()\n",
        "            valid_pixels = result[\"valid_mask\"].sum().item()\n",
        "            snow_percent = (snow_pixels / valid_pixels * 100) if valid_pixels > 0 else 0\n",
        "            \n",
        "            print(f\"  → Snow pixels: {snow_pixels:,} ({snow_percent:.1f}% of valid area)\")\n",
        "            \n",
        "            results.append(result)\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"  → Error: {e}\")\n",
        "            continue\n",
        "    \n",
        "    return results\n",
        "\n",
        "\n",
        "# Process scenes\n",
        "if hrefs:\n",
        "    print(f\"Starting file-based processing of {len(hrefs)} scene(s)...\\n\")\n",
        "    results = process_scenes(hrefs, x_slice, y_slice)\n",
        "    print(f\"\\n✓ Successfully processed {len(results)} scene(s)\")\n",
        "else:\n",
        "    print(\"No scenes to process.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
